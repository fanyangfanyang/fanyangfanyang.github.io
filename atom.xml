<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>努力学习NLP</title>
  
  
  <link href="/fanyangfanyang.github.io/atom.xml" rel="self"/>
  
  <link href="https://fanyangfanyang.github.io/blog/"/>
  <updated>2018-12-18T14:00:47.576Z</updated>
  <id>https://fanyangfanyang.github.io/blog/</id>
  
  <author>
    <name>Fan Yang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>结合源码理解Bert</title>
    <link href="https://fanyangfanyang.github.io/blog/2018/12/14/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3Bert/"/>
    <id>https://fanyangfanyang.github.io/blog/2018/12/14/结合源码理解Bert/</id>
    <published>2018-12-14T05:47:14.000Z</published>
    <updated>2018-12-18T14:00:47.576Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Bert</a>是谷歌于前段时间发布的一个预训练的语言模型，被誉为NLP领域的ImageNet。在共计11项NLP不同领域的评测任务中，Bert均取得了state-of-art的成绩。截至笔者写这篇文章时，<a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">squad2.0的leaderboard</a>前7名（包括ensemble和single model）已经全部被结合Bert方法的模型所占领。可以说，这一模型的出现，让大家都必须以一个全新的角度来考虑迁移学习对于NLP的重要性。或者正如许多知名学者所说的那样，Bert正在引领NLP进入一个全新的时代。<br>&emsp;&emsp;本文将从Bert模型的整体结构入手，并结合代码的实现细节来对其进行分析。</p><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>&emsp;&emsp;Transformer是由谷歌在<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is ALL You Need</a>一文提出的一种模型。它完全地抛弃了原先在NLP领域应用十分广泛的LSTM结构，转而仅利用Attention机制和全连接层来挖掘文本中蕴含的信息，具体结构如下图所示：<br><img src="https://ihipja.dm.files.1drv.com/y4m0NJNIXircPWfyWJhILvo8qu76K4T881uNi6Upfmx_3Lgzcn3pVraVOoi5dCheN8rjb1DK1vxYCylcm5Vpe4y0Jq6gjcf9f931BaLiZx_fYA2DHkAikZjnrnXewohZPBXJJfRrYgPN1wvC0r_8wpi3kWUEO1OUVOE4fCw0yUPQAgMaWMVpm-jSyfBYk5N00D3NEP6_spKpJp8YV4KiV9x7Q?width=525&amp;height=711&amp;cropmode=none" alt=""><br>&emsp;&emsp;由于不再使用RNN（LSTM）网络结构，这一模型摆脱了输入序列在先后关系上的约束，即可以同时输入与处理同一句子所有单词。这一点极大的提高了模型的并行性，缩短了模型训练所需要的时间。<br>&emsp;&emsp;这一模型最初被应用在机器翻译领域，并在WMT2014的英德翻译、英法翻译评测中，都取得了当时最好的的效果。<br>&emsp;&emsp;如今，包括<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">Bert</a>在内的许多模型，都采用了Transformer的思路，并取得了不错的成绩。具体的结构和代码细节将在下一节中进行详细阐述。</p><h1 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h1><h2 id="Transformer-Encoder-Block"><a href="#Transformer-Encoder-Block" class="headerlink" title="Transformer Encoder Block"></a>Transformer Encoder Block</h2><p>&emsp;&emsp;将Transformer结构应用在语言模型中，<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">Bert</a>并不是首例。以我所知，<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">OpenAI Transformer</a>则更早一些。<br>&emsp;&emsp;不过两者不同的是，OpenAI采用的是Transformer中的Decoder结构，通过Mask机制，遮掩住当前位置后面的词语，来从左到右依次预测句子中的每个词汇，而避免模型提前知道自己将要预测的内容。<br>&emsp;&emsp;而Bert采用的则是Transformer中的Encoder结构，通过最开始就完全遮掩住语料中15%的词汇（具体实现时并不是15%，细节后面会有介绍），然后用句子中的其他部分（包括上文和下文中未被遮掩的词汇）来预测被遮掩的部分。在计算loss时，Bert只统计被遮掩部分的预测结果。另外，Bert还提出了另一个任务，预测两个句子是否是连续存在于文章中的。<br>&emsp;&emsp;从模型整体来看，OpenAI是一个从左到右的单向模型，而Bert则是一个同时考虑上下文的真正的双向模型。从结果上来看，就目前而言，Bert取得了更好的效果。<br>&emsp;&emsp;在这一小节中，我们将着重介绍Bert中应用到的Transformer Encoder Block。<br>&emsp;&emsp;正如上一节中的Transformer结构图所示（左边的部分为Encoder），Encoder主要分为两层：Multi-Head Attention以及Feed Forward。</p><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>&emsp;&emsp;众所周知，Attention机制有三要素（V,K,Q），我们通过计算Query与Key的相似度，来获得一组与Value相对应的权重，并通过这一权重与Value相乘获得我们最后的结果。在这个模型中，我们采用self-attention机制，即V、K、Q均有相同的值（这一层的输入）。<br>&emsp;&emsp;在Transformer Encoder中采用的self-attention机制的结构主要如下图所示：<br><img src="https://ghj0fw.dm.files.1drv.com/y4myMFzeL2MbpDPzC7-KdiZKB9SPPDqGUV8TBkwoSRvzXo_zaKlIQnY8RROredd4hz7ZiglmwiS9CkJ-6eS44Q6Y2lIy1s24m9wGIwXDfrcMNWOCc8YzjzlqNUerrLD43gwFo5rJk8VKtVP0tdWharjD-YXJr8hpYNuA8iMFh5IKjI-SzcyL5ePVjKTbWGbh9XniKh1xVRYUQV6kKXvuKlULg?width=737&amp;height=387&amp;cropmode=none" alt=""><br>&emsp;&emsp;我们首先来解释一下右边的Multi-Head Attention。顾名思义，Multi-Head意味着我们使用同样的输入来多次计算self-attention的结果，来取得更好的成绩。在具体实现时，对于每一个head，我们将输入分别作为V、K、Q映射到size_per_head维的空间中，获得每一个head的结果。然后，将num_attention_heads个结果首尾相连，获得一个size_per_head*num_attention_heads维的结果。最后我们将这个结果映射回hidden_size维的空间。<br>&emsp;&emsp;而左边的Scaled Dot-Product Attention（点乘注意力机制）所展示的，则是右边图中每一个head的具体实现。文中通过对Query和Key进行相乘来获得相似度，然后对其结果进行softmax来获得Value每一维度的权重，将权重与Value相乘获得最后的结果。这里的Mask机制在Transformer的Decoder中用到，而在本文中的Encoder中并没有实际用途，因此不做介绍。<br>&emsp;&emsp;代码实现如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              ...</span><br><span class="line">              )</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;attention_layer的调用，其中from_tensor和to_tensor都传入的是attention层的输入。这对应我们上文中提到的Q、K、V的值相同。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">to_tensor_2d = reshape_to_matrix(to_tensor)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;将from_tensor和to_tensor（同一个东西）由[batch_size, sequence_length, hidden_size]的维度展开成[batch_size*sequence_length, hidden_size]的二维矩阵。这对应着我们上文提到的，Transformer模型中每一个Batch里所有句子的所有的词可以同时进行计算，以此来节约大量的时间<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Scalar dimensions referenced here:</span><br><span class="line">#   B = batch size (number of sequences)</span><br><span class="line">#   F = `from_tensor` sequence length</span><br><span class="line">#   T = `to_tensor` sequence length</span><br><span class="line">#   N = `num_attention_heads`</span><br><span class="line">#   H = `size_per_head`</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;这是一些数值的简写，在下文中用来表示数据的维度，其中F=T。每一个数值的意思都十分明显，这里不做过多介绍。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># `query_layer` = [B*F, N*H]</span><br><span class="line">query_layer = tf.layers.dense(</span><br><span class="line">    from_tensor_2d,</span><br><span class="line">    num_attention_heads * size_per_head,</span><br><span class="line">    ...</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"># `key_layer` = [B*T, N*H]</span><br><span class="line">key_layer = tf.layers.dense(</span><br><span class="line">    to_tensor_2d,</span><br><span class="line">    num_attention_heads * size_per_head,</span><br><span class="line">    ...</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"># `value_layer` = [B*T, N*H]</span><br><span class="line">value_layer = tf.layers.dense(</span><br><span class="line">    to_tensor_2d,</span><br><span class="line">    num_attention_heads * size_per_head,</span><br><span class="line">    ...</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;这是对Q、K、V分别进行映射。虽然只是一次计算，但它代表着num_attention_heads次将Q、K、V分别映射到size_per_head维的空间中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># `query_layer` = [B, N, F, H]</span><br><span class="line">query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                   num_attention_heads, from_seq_length,</span><br><span class="line">                                   size_per_head)</span><br><span class="line"></span><br><span class="line"># `key_layer` = [B, N, T, H]</span><br><span class="line">key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                 to_seq_length, size_per_head)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;这是为了方便下面的点乘计算相似度，而将Query和Key的值进行转置。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># `attention_scores` = [B, N, F, T]</span><br><span class="line">attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)</span><br><span class="line">attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                               1.0 / math.sqrt(float(size_per_head)))</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;attention_scores就是通过Q与K的乘法计算出的相似度。然后对其的每一维都除以size_per_head的平方根。论文中说第二步处理可以使模型获得更好的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># `attention_probs` = [B, N, F, T]</span><br><span class="line">attention_probs = tf.nn.softmax(attention_scores)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;通过softmax方法将Key与Query的相似度归一化为Value的权重。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># `value_layer` = [B, T, N, H]</span><br><span class="line">value_layer = tf.reshape(</span><br><span class="line">    value_layer,</span><br><span class="line">    [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line"># `value_layer` = [B, N, T, H]</span><br><span class="line">value_layer = tf.transpose(value_layer, [0, 2, 1, 3])</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;对Value进行转置来方便它和权重的点乘计算。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># `context_layer` = [B, N, F, H]</span><br><span class="line">context_layer = tf.matmul(attention_probs, value_layer)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;将Value和之前获得的权重进行点乘，计算出每一个head的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># `context_layer` = [B, F, N, H]</span><br><span class="line">context_layer = tf.transpose(context_layer, [0, 2, 1, 3])</span><br><span class="line"># `context_layer` = [B*F, N*V]</span><br><span class="line">context_layer = tf.reshape(</span><br><span class="line">  context_layer,</span><br><span class="line">  [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;将每一个head的结果首尾相接，连成num_attention_heads*size_per_head维的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              ...)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;将attention层的结果由num_attention_heads*size_per_head维映射回hidden_size维。<br>&emsp;&emsp;至此，我们将Encoder中的Mulit-Head Attention层的工作流程介绍完毕。</p><h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h3><p>&emsp;&emsp;相较于attention层而言，FFNN层的结构则要简单的多，在本模型中，FFNN层的输入（即attention层的输出）被传入一个隐层节点数为3072的全连接网络中，最终以统一的hidden_size为维度输出。值得一提的是，隐层使用的激活函数是<a href="https://arxiv.org/pdf/1606.08415.pdf" target="_blank" rel="noopener">Gelu</a>，这是relu函数的一种更为平滑的版本，输出的结果是标准正态分布的累积分布函数值与输入值的乘积。<br>&emsp;&emsp;代码实现如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))</span><br><span class="line">return input_tensor * cdf</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;这是gelu函数的代码，input_tensor是函数的输入，erf是误差函数，cdf是标准正态分布的累积分布函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">intermediate_output = tf.layers.dense(</span><br><span class="line">    attention_output,</span><br><span class="line">    intermediate_size,</span><br><span class="line">    activation=intermediate_act_fn,</span><br><span class="line">    kernel_initializer=create_initializer(initializer_range))</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;这是节点数为intermediate_siza（3072），激活函数为gelu的隐层。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;这是节点数为hidden_size的输出层。</p><h3 id="Add-amp-Norm-残差连接和层正则化"><a href="#Add-amp-Norm-残差连接和层正则化" class="headerlink" title="Add&amp;Norm 残差连接和层正则化"></a>Add&amp;Norm 残差连接和层正则化</h3><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">layer normalization</a>是<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch Normalization</a>的一种优化版本，具体原理可以查阅论文得知，在此不过多赘述。<br>&emsp;&emsp;代码实现如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def layer_norm(input_tensor, name=None):</span><br><span class="line">  &quot;&quot;&quot;Run layer normalization on the last dimension of the tensor.&quot;&quot;&quot;</span><br><span class="line">  return tf.contrib.layers.layer_norm(</span><br><span class="line">      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;这是本模型中的layer_norm的函数实现，可以看见主要是调用了tensorflow的layer_norm()函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">attention_output = layer_norm(attention_output + layer_input)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;这是attention层后的残差连接和层正则化。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layer_output = layer_norm(layer_output + attention_output)</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;这是FFNN层后的残差连接和层正则化。</p><h2 id="Input-Embedding"><a href="#Input-Embedding" class="headerlink" title="Input Embedding"></a>Input Embedding</h2><p>&emsp;&emsp;本模型中的Input Embedding由下图中的三部分组成：<br><img src="https://gbj0fw.dm.files.1drv.com/y4mq6s0Oo0YyDtapQiukmzVWLIMgdoxMhy9Xl_dpGdoNb7g4diA98caGRZCjLcSda4NOoBfS6rJ3NTK7dP1Ld02_2eje3muzOQKoJSWo5PdwAKZ_pGlrJ-t0SYQ7FBiEGYxGnU_druRX8j16V3r5VddiJGpDZQTZ4zxKsXJ2d3yu3itUSfBLvRl8UwOuwz6hxW2ApJAHaZPws5Mli6fyvGqJg?width=1009&amp;height=362&amp;cropmode=none" alt=""><br>&emsp;&emsp;第一部分为普通的word embedding，由训练得到；第二部分为segment emdedding，因为此模型的输入有可能是不同数量的句子，我们使用这一部分来区分不同的句子，同样由训练得出；第三部分为position embedding，由于此模型没有使用RNN网络，因此，为了保存词语之间的先后顺序关系，我们使用这一部分来记录每一个词语在输入中的相对位置。</p><h2 id="Pre-Training-Tasks"><a href="#Pre-Training-Tasks" class="headerlink" title="Pre-Training Tasks"></a>Pre-Training Tasks</h2><p>&emsp;&emsp;Bert预训练过程的loss函数主要由两个部分组成：</p><h3 id="Masked-LM"><a href="#Masked-LM" class="headerlink" title="Masked LM"></a>Masked LM</h3><p>&emsp;&emsp;在此语言模型中，为了不提前“泄露”需要预测的内容，在数据预处理部分，就Mask了15%的词汇，作为需要预测的目标。针对这15%的词汇，做出一下处理：</p><ol><li>80%的情况下，用[mask]来代替选中的词语，如 my dog is hairy →m y dog is [MASK]；</li><li>10%的情况下，用一个随机词语来代替选中的词语，如 my dog is hairy → mydog is apple；</li><li>10%的情况下，选中的词语维持不变，如 my dog is hairy → my dogi s hairy。</li></ol><p>&emsp;&emsp;第2，3种情况是为了防止模型对[mask]字符产生依赖，变得只有看见[mask]字符时才能准确预测结果，因为面对具体任务的输入中并不会由[mask]字符。<br>&emsp;&emsp;在loss函数中，只有这15%的被选中的单词的预测的准确率会被统计。</p><h3 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h3><p>&emsp;&emsp;在预训练的过程中，每个输入包含2个句子：句子A和句子B。在50%的情况下，句子B是句子A的下一个句子；在50%的情况下，句子B是一个随机的句子。<br>&emsp;&emsp;在这一任务中，我们通过预测句子间的连接关系，来学习句子内的逻辑联系。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;&amp;emsp;&amp;emsp;&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot; target=&quot;_blank&quot;
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://fanyangfanyang.github.io/blog/tags/NLP/"/>
    
      <category term="Bert" scheme="https://fanyangfanyang.github.io/blog/tags/Bert/"/>
    
  </entry>
  
  <entry>
    <title>使用Hexo+Github搭建个人博客</title>
    <link href="https://fanyangfanyang.github.io/blog/2018/12/13/%E4%BD%BF%E7%94%A8Hexo+Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>https://fanyangfanyang.github.io/blog/2018/12/13/使用Hexo+Github搭建个人博客/</id>
    <published>2018-12-13T15:34:55.000Z</published>
    <updated>2018-12-14T09:28:12.182Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一些参考资料"><a href="#一些参考资料" class="headerlink" title="一些参考资料"></a>一些参考资料</h1><p>&emsp;&emsp;由于本人也是这方面的萌新，所以主要过程都参考的知乎：<a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">GitHub+Hexo 搭建个人网站详细教程</a>这篇博客内容很丰富，过程也比较具体，这里就不重复说了。</p><p>&emsp;&emsp;我使用的主题是<a href="https://chaoo.oschina.io/2016/12/29/BlueLake%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98%E7%9A%84%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE.html" target="_blank" rel="noopener">BlueLake</a>具体配置方法在上面的教程中也比较详细，这里也不再赘述。</p><p>&emsp;&emsp;<a href="https://markdown-zh.readthedocs.io/en/latest/blockelements/" target="_blank" rel="noopener">MarkDown中文文档</a></p><p>&emsp;&emsp;<a href="https://dillinger.io/" target="_blank" rel="noopener">很好用的Markdown在线编辑网站dillinger</a></p><p>&emsp;&emsp;<a href="https://segmentfault.com/markdown" target="_blank" rel="noopener">很方便的Markdown语法工具书</a></p><h1 id="遇到的几个问题："><a href="#遇到的几个问题：" class="headerlink" title="遇到的几个问题："></a>遇到的几个问题：</h1><h2 id="1、配置后网页无法显示主题"><a href="#1、配置后网页无法显示主题" class="headerlink" title="1、配置后网页无法显示主题"></a>1、配置后网页无法显示主题</h2><p>&emsp;&emsp;在更改网站的主题后，生成网页并本地成功预览，然而当完成部署后（并没有报错）,博客网站加载主题格式失败，网页布局混乱。</p><p><strong>解决方法：</strong><br>&emsp;&emsp;在站点配置文件（根目录下的_config.yml）中，更改url和root两个属性的值，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">url: http://fanyangfanyang.github.io/blog</span><br><span class="line">root: /fanyangfanyang.github.io/</span><br></pre></td></tr></table></figure><h2 id="2、无法连接github库"><a href="#2、无法连接github库" class="headerlink" title="2、无法连接github库"></a>2、无法连接github库</h2><p>&emsp;&emsp;在使用hexo d进行配置时报错，显示github库连接错误</p><p><strong>解决方法：</strong><br>&emsp;&emsp;根目录下的_config.yml文件中的deploy: repo:属性的网址前缀不能使用https。改为http后，错误解决。<br>（似乎还同时解决了文章开头的属性被识别为markdown语言的问题。）</p><h2 id="3、段落开头的缩进问题"><a href="#3、段落开头的缩进问题" class="headerlink" title="3、段落开头的缩进问题"></a>3、段落开头的缩进问题</h2><p>&emsp;&emsp;大概是由于英语中没有缩进的习惯，所以Markdown中你并不能使用空格进行方便的缩进。</p><p><strong>解决方法：</strong><br>&emsp;&emsp;可以使用下面三种字符进行首行缩进：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&amp;emsp; //相当于一个中文字符的空格</span><br><span class="line">&amp;ensp; //相当于一个英文字符（半个中文字符）的空格</span><br><span class="line">&amp;nbsp; //相当于半个英文字符的空格</span><br></pre></td></tr></table></figure></p><p><em>本人的第一篇博客O(∩_∩)O</em></p><p><em>未完待续。。</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一些参考资料&quot;&gt;&lt;a href=&quot;#一些参考资料&quot; class=&quot;headerlink&quot; title=&quot;一些参考资料&quot;&gt;&lt;/a&gt;一些参考资料&lt;/h1&gt;&lt;p&gt;&amp;emsp;&amp;emsp;由于本人也是这方面的萌新，所以主要过程都参考的知乎：&lt;a href=&quot;https:/
      
    
    </summary>
    
    
      <category term="hexo" scheme="https://fanyangfanyang.github.io/blog/tags/hexo/"/>
    
      <category term="Github" scheme="https://fanyangfanyang.github.io/blog/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://fanyangfanyang.github.io/blog/2018/12/13/hello-world/"/>
    <id>https://fanyangfanyang.github.io/blog/2018/12/13/hello-world/</id>
    <published>2018-12-13T11:31:02.412Z</published>
    <updated>2018-12-13T11:31:02.412Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
