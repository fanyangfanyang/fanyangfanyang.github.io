{"meta":{"title":"努力学习NLP","subtitle":null,"description":"苏老泉，二十七，始发奋，读书籍","author":"Fan Yang","url":"https://fanyangfanyang.github.io/blog"},"pages":[{"title":"about","date":"2018-12-13T13:10:03.000Z","updated":"2018-12-13T13:10:03.925Z","comments":true,"path":"about/index.html","permalink":"https://fanyangfanyang.github.io/blog/about/index.html","excerpt":"","text":""}],"posts":[{"title":"结合源码理解Bert","slug":"结合源码理解Bert","date":"2018-12-14T05:47:14.000Z","updated":"2018-12-18T14:00:47.576Z","comments":true,"path":"2018/12/14/结合源码理解Bert/","link":"","permalink":"https://fanyangfanyang.github.io/blog/2018/12/14/结合源码理解Bert/","excerpt":"","text":"摘要&emsp;&emsp;Bert是谷歌于前段时间发布的一个预训练的语言模型，被誉为NLP领域的ImageNet。在共计11项NLP不同领域的评测任务中，Bert均取得了state-of-art的成绩。截至笔者写这篇文章时，squad2.0的leaderboard前7名（包括ensemble和single model）已经全部被结合Bert方法的模型所占领。可以说，这一模型的出现，让大家都必须以一个全新的角度来考虑迁移学习对于NLP的重要性。或者正如许多知名学者所说的那样，Bert正在引领NLP进入一个全新的时代。&emsp;&emsp;本文将从Bert模型的整体结构入手，并结合代码的实现细节来对其进行分析。 Transformer&emsp;&emsp;Transformer是由谷歌在Attention Is ALL You Need一文提出的一种模型。它完全地抛弃了原先在NLP领域应用十分广泛的LSTM结构，转而仅利用Attention机制和全连接层来挖掘文本中蕴含的信息，具体结构如下图所示：&emsp;&emsp;由于不再使用RNN（LSTM）网络结构，这一模型摆脱了输入序列在先后关系上的约束，即可以同时输入与处理同一句子所有单词。这一点极大的提高了模型的并行性，缩短了模型训练所需要的时间。&emsp;&emsp;这一模型最初被应用在机器翻译领域，并在WMT2014的英德翻译、英法翻译评测中，都取得了当时最好的的效果。&emsp;&emsp;如今，包括Bert在内的许多模型，都采用了Transformer的思路，并取得了不错的成绩。具体的结构和代码细节将在下一节中进行详细阐述。 BertTransformer Encoder Block&emsp;&emsp;将Transformer结构应用在语言模型中，Bert并不是首例。以我所知，OpenAI Transformer则更早一些。&emsp;&emsp;不过两者不同的是，OpenAI采用的是Transformer中的Decoder结构，通过Mask机制，遮掩住当前位置后面的词语，来从左到右依次预测句子中的每个词汇，而避免模型提前知道自己将要预测的内容。&emsp;&emsp;而Bert采用的则是Transformer中的Encoder结构，通过最开始就完全遮掩住语料中15%的词汇（具体实现时并不是15%，细节后面会有介绍），然后用句子中的其他部分（包括上文和下文中未被遮掩的词汇）来预测被遮掩的部分。在计算loss时，Bert只统计被遮掩部分的预测结果。另外，Bert还提出了另一个任务，预测两个句子是否是连续存在于文章中的。&emsp;&emsp;从模型整体来看，OpenAI是一个从左到右的单向模型，而Bert则是一个同时考虑上下文的真正的双向模型。从结果上来看，就目前而言，Bert取得了更好的效果。&emsp;&emsp;在这一小节中，我们将着重介绍Bert中应用到的Transformer Encoder Block。&emsp;&emsp;正如上一节中的Transformer结构图所示（左边的部分为Encoder），Encoder主要分为两层：Multi-Head Attention以及Feed Forward。 Multi-Head Attention&emsp;&emsp;众所周知，Attention机制有三要素（V,K,Q），我们通过计算Query与Key的相似度，来获得一组与Value相对应的权重，并通过这一权重与Value相乘获得我们最后的结果。在这个模型中，我们采用self-attention机制，即V、K、Q均有相同的值（这一层的输入）。&emsp;&emsp;在Transformer Encoder中采用的self-attention机制的结构主要如下图所示：&emsp;&emsp;我们首先来解释一下右边的Multi-Head Attention。顾名思义，Multi-Head意味着我们使用同样的输入来多次计算self-attention的结果，来取得更好的成绩。在具体实现时，对于每一个head，我们将输入分别作为V、K、Q映射到size_per_head维的空间中，获得每一个head的结果。然后，将num_attention_heads个结果首尾相连，获得一个size_per_head*num_attention_heads维的结果。最后我们将这个结果映射回hidden_size维的空间。&emsp;&emsp;而左边的Scaled Dot-Product Attention（点乘注意力机制）所展示的，则是右边图中每一个head的具体实现。文中通过对Query和Key进行相乘来获得相似度，然后对其结果进行softmax来获得Value每一维度的权重，将权重与Value相乘获得最后的结果。这里的Mask机制在Transformer的Decoder中用到，而在本文中的Encoder中并没有实际用途，因此不做介绍。&emsp;&emsp;代码实现如下所示：12345attention_head = attention_layer( from_tensor=layer_input, to_tensor=layer_input, ... ) &emsp;&emsp;attention_layer的调用，其中from_tensor和to_tensor都传入的是attention层的输入。这对应我们上文中提到的Q、K、V的值相同。12from_tensor_2d = reshape_to_matrix(from_tensor)to_tensor_2d = reshape_to_matrix(to_tensor) &emsp;&emsp;将from_tensor和to_tensor（同一个东西）由[batch_size, sequence_length, hidden_size]的维度展开成[batch_size*sequence_length, hidden_size]的二维矩阵。这对应着我们上文提到的，Transformer模型中每一个Batch里所有句子的所有的词可以同时进行计算，以此来节约大量的时间123456# Scalar dimensions referenced here:# B = batch size (number of sequences)# F = `from_tensor` sequence length# T = `to_tensor` sequence length# N = `num_attention_heads`# H = `size_per_head` &emsp;&emsp;这是一些数值的简写，在下文中用来表示数据的维度，其中F=T。每一个数值的意思都十分明显，这里不做过多介绍。1234567891011121314151617181920# `query_layer` = [B*F, N*H]query_layer = tf.layers.dense( from_tensor_2d, num_attention_heads * size_per_head, ... )# `key_layer` = [B*T, N*H]key_layer = tf.layers.dense( to_tensor_2d, num_attention_heads * size_per_head, ... )# `value_layer` = [B*T, N*H]value_layer = tf.layers.dense( to_tensor_2d, num_attention_heads * size_per_head, ... ) &emsp;&emsp;这是对Q、K、V分别进行映射。虽然只是一次计算，但它代表着num_attention_heads次将Q、K、V分别映射到size_per_head维的空间中。12345678# `query_layer` = [B, N, F, H]query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads, from_seq_length, size_per_head)# `key_layer` = [B, N, T, H]key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, to_seq_length, size_per_head) &emsp;&emsp;这是为了方便下面的点乘计算相似度，而将Query和Key的值进行转置。1234# `attention_scores` = [B, N, F, T]attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head))) &emsp;&emsp;attention_scores就是通过Q与K的乘法计算出的相似度。然后对其的每一维都除以size_per_head的平方根。论文中说第二步处理可以使模型获得更好的结果。12# `attention_probs` = [B, N, F, T]attention_probs = tf.nn.softmax(attention_scores) &emsp;&emsp;通过softmax方法将Key与Query的相似度归一化为Value的权重。1234567# `value_layer` = [B, T, N, H]value_layer = tf.reshape( value_layer, [batch_size, to_seq_length, num_attention_heads, size_per_head])# `value_layer` = [B, N, T, H]value_layer = tf.transpose(value_layer, [0, 2, 1, 3]) &emsp;&emsp;对Value进行转置来方便它和权重的点乘计算。12# `context_layer` = [B, N, F, H]context_layer = tf.matmul(attention_probs, value_layer) &emsp;&emsp;将Value和之前获得的权重进行点乘，计算出每一个head的结果。123456# `context_layer` = [B, F, N, H]context_layer = tf.transpose(context_layer, [0, 2, 1, 3])# `context_layer` = [B*F, N*V]context_layer = tf.reshape( context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head]) &emsp;&emsp;将每一个head的结果首尾相接，连成num_attention_heads*size_per_head维的结果。1234attention_output = tf.layers.dense( attention_output, hidden_size, ...) &emsp;&emsp;将attention层的结果由num_attention_heads*size_per_head维映射回hidden_size维。&emsp;&emsp;至此，我们将Encoder中的Mulit-Head Attention层的工作流程介绍完毕。 Feed Forward&emsp;&emsp;相较于attention层而言，FFNN层的结构则要简单的多，在本模型中，FFNN层的输入（即attention层的输出）被传入一个隐层节点数为3072的全连接网络中，最终以统一的hidden_size为维度输出。值得一提的是，隐层使用的激活函数是Gelu，这是relu函数的一种更为平滑的版本，输出的结果是标准正态分布的累积分布函数值与输入值的乘积。&emsp;&emsp;代码实现如下所示：12cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))return input_tensor * cdf &emsp;&emsp;这是gelu函数的代码，input_tensor是函数的输入，erf是误差函数，cdf是标准正态分布的累积分布函数。12345intermediate_output = tf.layers.dense( attention_output, intermediate_size, activation=intermediate_act_fn, kernel_initializer=create_initializer(initializer_range)) &emsp;&emsp;这是节点数为intermediate_siza（3072），激活函数为gelu的隐层。1234layer_output = tf.layers.dense( intermediate_output, hidden_size, kernel_initializer=create_initializer(initializer_range)) &emsp;&emsp;这是节点数为hidden_size的输出层。 Add&amp;Norm 残差连接和层正则化&emsp;&emsp;layer normalization是Batch Normalization的一种优化版本，具体原理可以查阅论文得知，在此不过多赘述。&emsp;&emsp;代码实现如下所示：1234def layer_norm(input_tensor, name=None): &quot;&quot;&quot;Run layer normalization on the last dimension of the tensor.&quot;&quot;&quot; return tf.contrib.layers.layer_norm( inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name) &emsp;&emsp;这是本模型中的layer_norm的函数实现，可以看见主要是调用了tensorflow的layer_norm()函数。1attention_output = layer_norm(attention_output + layer_input) &emsp;&emsp;这是attention层后的残差连接和层正则化。1layer_output = layer_norm(layer_output + attention_output) &emsp;&emsp;这是FFNN层后的残差连接和层正则化。 Input Embedding&emsp;&emsp;本模型中的Input Embedding由下图中的三部分组成：&emsp;&emsp;第一部分为普通的word embedding，由训练得到；第二部分为segment emdedding，因为此模型的输入有可能是不同数量的句子，我们使用这一部分来区分不同的句子，同样由训练得出；第三部分为position embedding，由于此模型没有使用RNN网络，因此，为了保存词语之间的先后顺序关系，我们使用这一部分来记录每一个词语在输入中的相对位置。 Pre-Training Tasks&emsp;&emsp;Bert预训练过程的loss函数主要由两个部分组成： Masked LM&emsp;&emsp;在此语言模型中，为了不提前“泄露”需要预测的内容，在数据预处理部分，就Mask了15%的词汇，作为需要预测的目标。针对这15%的词汇，做出一下处理： 80%的情况下，用[mask]来代替选中的词语，如 my dog is hairy →m y dog is [MASK]； 10%的情况下，用一个随机词语来代替选中的词语，如 my dog is hairy → mydog is apple； 10%的情况下，选中的词语维持不变，如 my dog is hairy → my dogi s hairy。 &emsp;&emsp;第2，3种情况是为了防止模型对[mask]字符产生依赖，变得只有看见[mask]字符时才能准确预测结果，因为面对具体任务的输入中并不会由[mask]字符。&emsp;&emsp;在loss函数中，只有这15%的被选中的单词的预测的准确率会被统计。 Next Sentence Prediction&emsp;&emsp;在预训练的过程中，每个输入包含2个句子：句子A和句子B。在50%的情况下，句子B是句子A的下一个句子；在50%的情况下，句子B是一个随机的句子。&emsp;&emsp;在这一任务中，我们通过预测句子间的连接关系，来学习句子内的逻辑联系。","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://fanyangfanyang.github.io/blog/tags/NLP/"},{"name":"Bert","slug":"Bert","permalink":"https://fanyangfanyang.github.io/blog/tags/Bert/"}],"keywords":[]},{"title":"使用Hexo+Github搭建个人博客","slug":"使用Hexo+Github搭建个人博客","date":"2018-12-13T15:34:55.000Z","updated":"2018-12-14T09:28:12.182Z","comments":true,"path":"2018/12/13/使用Hexo+Github搭建个人博客/","link":"","permalink":"https://fanyangfanyang.github.io/blog/2018/12/13/使用Hexo+Github搭建个人博客/","excerpt":"","text":"一些参考资料&emsp;&emsp;由于本人也是这方面的萌新，所以主要过程都参考的知乎：GitHub+Hexo 搭建个人网站详细教程这篇博客内容很丰富，过程也比较具体，这里就不重复说了。 &emsp;&emsp;我使用的主题是BlueLake具体配置方法在上面的教程中也比较详细，这里也不再赘述。 &emsp;&emsp;MarkDown中文文档 &emsp;&emsp;很好用的Markdown在线编辑网站dillinger &emsp;&emsp;很方便的Markdown语法工具书 遇到的几个问题：1、配置后网页无法显示主题&emsp;&emsp;在更改网站的主题后，生成网页并本地成功预览，然而当完成部署后（并没有报错）,博客网站加载主题格式失败，网页布局混乱。 解决方法：&emsp;&emsp;在站点配置文件（根目录下的_config.yml）中，更改url和root两个属性的值，如下所示： 12url: http://fanyangfanyang.github.io/blogroot: /fanyangfanyang.github.io/ 2、无法连接github库&emsp;&emsp;在使用hexo d进行配置时报错，显示github库连接错误 解决方法：&emsp;&emsp;根目录下的_config.yml文件中的deploy: repo:属性的网址前缀不能使用https。改为http后，错误解决。（似乎还同时解决了文章开头的属性被识别为markdown语言的问题。） 3、段落开头的缩进问题&emsp;&emsp;大概是由于英语中没有缩进的习惯，所以Markdown中你并不能使用空格进行方便的缩进。 解决方法：&emsp;&emsp;可以使用下面三种字符进行首行缩进：123&amp;emsp; //相当于一个中文字符的空格&amp;ensp; //相当于一个英文字符（半个中文字符）的空格&amp;nbsp; //相当于半个英文字符的空格 本人的第一篇博客O(∩_∩)O 未完待续。。","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://fanyangfanyang.github.io/blog/tags/hexo/"},{"name":"Github","slug":"Github","permalink":"https://fanyangfanyang.github.io/blog/tags/Github/"}],"keywords":[]},{"title":"Hello World","slug":"hello-world","date":"2018-12-13T11:31:02.412Z","updated":"2018-12-13T11:31:02.412Z","comments":true,"path":"2018/12/13/hello-world/","link":"","permalink":"https://fanyangfanyang.github.io/blog/2018/12/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[],"keywords":[]}]}