<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="苏老泉，二十七，始发奋，读书籍"><title>结合源码理解Bert | 努力学习NLP</title><link rel="stylesheet" type="text/css" href="//fonts.neworld.org/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/fanyangfanyang.github.io/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/fanyangfanyang.github.io/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/fanyangfanyang.github.io/favicon.ico"><link rel="bookmark" href="/fanyangfanyang.github.io/favicon.ico"><link rel="apple-touch-icon" href="/fanyangfanyang.github.io/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/fanyangfanyang.github.io/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/fanyangfanyang.github.io/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">结合源码理解Bert</h1><a id="logo" href="/fanyangfanyang.github.io/.">努力学习NLP</a><p class="description"></p></div><div id="nav-menu"><a href="/fanyangfanyang.github.io/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/fanyangfanyang.github.io/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/fanyangfanyang.github.io/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">结合源码理解Bert</h1><div class="post-meta"><a href="/fanyangfanyang.github.io/2018/12/14/结合源码理解Bert/#comments" class="comment-count"></a><p><span class="date">Dec 14, 2018</span></p></div><div class="post-content"><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>&emsp;&emsp;<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Bert</a>是谷歌于前段时间发布的一个预训练的语言模型，被誉为NLP领域的ImageNet。在共计11项NLP不同领域的评测任务中，Bert均取得了state-of-art的成绩。截至笔者写这篇文章时，<a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">squad2.0的leaderboard</a>前7名（包括ensemble和single model）已经全部被结合Bert方法的模型所占领。可以说，这一模型的出现，让大家都必须以一个全新的角度来考虑迁移学习对于NLP的重要性。或者正如许多知名学者所说的那样，Bert正在引领NLP进入一个全新的时代。<br>&emsp;&emsp;本文将从Bert模型的整体结构入手，并结合代码的实现细节来对其进行分析。</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>&emsp;&emsp;Transformer是由谷歌在<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is ALL You Need</a>一文提出的一种模型。它完全地抛弃了原先在NLP领域应用十分广泛的LSTM结构，转而仅利用Attention机制和全连接层来挖掘文本中蕴含的信息，具体结构如下图所示：<br><img src="https://ihipja.dm.files.1drv.com/y4m0NJNIXircPWfyWJhILvo8qu76K4T881uNi6Upfmx_3Lgzcn3pVraVOoi5dCheN8rjb1DK1vxYCylcm5Vpe4y0Jq6gjcf9f931BaLiZx_fYA2DHkAikZjnrnXewohZPBXJJfRrYgPN1wvC0r_8wpi3kWUEO1OUVOE4fCw0yUPQAgMaWMVpm-jSyfBYk5N00D3NEP6_spKpJp8YV4KiV9x7Q?width=525&amp;height=711&amp;cropmode=none" alt=""><br>&emsp;&emsp;由于不再使用RNN（LSTM）网络结构，这一模型摆脱了输入序列在先后关系上的约束，即可以同时输入与处理同一句子所有单词。这一点极大的提高了模型的并行性，缩短了模型训练所需要的时间。<br>&emsp;&emsp;这一模型最初被应用在机器翻译领域，并在WMT2014的英德翻译、英法翻译评测中，都取得了当时最好的的效果。<br>&emsp;&emsp;如今，包括<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">Bert</a>在内的许多模型，都采用了Transformer的思路，并取得了不错的成绩。具体的结构和代码细节将在下一节中进行详细阐述。</p>
<h1 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h1><h2 id="Transformer-Encoder-Block"><a href="#Transformer-Encoder-Block" class="headerlink" title="Transformer Encoder Block"></a>Transformer Encoder Block</h2><p>&emsp;&emsp;将Transformer结构应用在语言模型中，<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">Bert</a>并不是首例。以我所知，<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">OpenAI Transformer</a>则更早一些。<br>&emsp;&emsp;不过两者不同的是，OpenAI采用的是Transformer中的Decoder结构，通过Mask机制，遮掩住当前位置后面的词语，来从左到右依次预测句子中的每个词汇，而避免模型提前知道自己将要预测的内容。<br>&emsp;&emsp;而Bert采用的则是Transformer中的Encoder结构，通过最开始就完全遮掩住语料中15%的词汇（具体实现时并不是15%，细节后面会有介绍），然后用句子中的其他部分（包括上文和下文中未被遮掩的词汇）来预测被遮掩的部分。在计算loss时，Bert只统计被遮掩部分的预测结果。另外，Bert还提出了另一个任务，预测两个句子是否是连续存在于文章中的。<br>&emsp;&emsp;从模型整体来看，OpenAI是一个从左到右的单向模型，而Bert则是一个同时考虑上下文的真正的双向模型。从结果上来看，就目前而言，Bert取得了更好的效果。<br>&emsp;&emsp;在这一小节中，我们将着重介绍Bert中应用到的Transformer Encoder Block。<br>&emsp;&emsp;正如上一节中的Transformer结构图所示（左边的部分为Encoder），Encoder主要分为两层：Multi-Head Attention以及Feed Forward。</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>&emsp;&emsp;众所周知，Attention机制有三要素（V,K,Q），我们通过计算Query与Key的相似度，来获得一组与Value相对应的权重，并通过这一权重与Value相乘获得我们最后的结果。在这个模型中，我们采用self-attention机制，即V、K、Q均有相同的值（这一层的输入）。<br>&emsp;&emsp;在Transformer Encoder中采用的self-attention机制的结构主要如下图所示：<br><img src="https://ghj0fw.dm.files.1drv.com/y4myMFzeL2MbpDPzC7-KdiZKB9SPPDqGUV8TBkwoSRvzXo_zaKlIQnY8RROredd4hz7ZiglmwiS9CkJ-6eS44Q6Y2lIy1s24m9wGIwXDfrcMNWOCc8YzjzlqNUerrLD43gwFo5rJk8VKtVP0tdWharjD-YXJr8hpYNuA8iMFh5IKjI-SzcyL5ePVjKTbWGbh9XniKh1xVRYUQV6kKXvuKlULg?width=737&amp;height=387&amp;cropmode=none" alt=""><br>&emsp;&emsp;我们首先来解释一下右边的Multi-Head Attention。顾名思义，Multi-Head意味着我们使用同样的输入来多次计算self-attention的结果，来取得更好的成绩。在具体实现时，对于每一个head，我们将输入分别作为V、K、Q映射到size_per_head维的空间中，获得每一个head的结果。然后，将num_attention_heads个结果首尾相连，获得一个size_per_head*num_attention_heads维的结果。最后我们将这个结果映射回hidden_size维的空间。<br>&emsp;&emsp;而左边的Scaled Dot-Product Attention（点乘注意力机制）所展示的，则是右边图中每一个head的具体实现。文中通过对Query和Key进行相乘来获得相似度，然后对其结果进行softmax来获得Value每一维度的权重，将权重与Value相乘获得最后的结果。这里的Mask机制在Transformer的Decoder中用到，而在本文中的Encoder中并没有实际用途，因此不做介绍。<br>&emsp;&emsp;代码实现如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              ...</span><br><span class="line">              )</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;attention_layer的调用，其中from_tensor和to_tensor都传入的是attention层的输入。这对应我们上文中提到的Q、K、V的值相同。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">to_tensor_2d = reshape_to_matrix(to_tensor)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;将from_tensor和to_tensor（同一个东西）由[batch_size, sequence_length, hidden_size]的维度展开成[batch_size*sequence_length, hidden_size]的二维矩阵。这对应着我们上文提到的，Transformer模型中每一个Batch里所有句子的所有的词可以同时进行计算，以此来节约大量的时间<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Scalar dimensions referenced here:</span><br><span class="line">#   B = batch size (number of sequences)</span><br><span class="line">#   F = `from_tensor` sequence length</span><br><span class="line">#   T = `to_tensor` sequence length</span><br><span class="line">#   N = `num_attention_heads`</span><br><span class="line">#   H = `size_per_head`</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这是一些数值的简写，在下文中用来表示数据的维度，其中F=T。每一个数值的意思都十分明显，这里不做过多介绍。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># `query_layer` = [B*F, N*H]</span><br><span class="line">query_layer = tf.layers.dense(</span><br><span class="line">    from_tensor_2d,</span><br><span class="line">    num_attention_heads * size_per_head,</span><br><span class="line">    ...</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"># `key_layer` = [B*T, N*H]</span><br><span class="line">key_layer = tf.layers.dense(</span><br><span class="line">    to_tensor_2d,</span><br><span class="line">    num_attention_heads * size_per_head,</span><br><span class="line">    ...</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"># `value_layer` = [B*T, N*H]</span><br><span class="line">value_layer = tf.layers.dense(</span><br><span class="line">    to_tensor_2d,</span><br><span class="line">    num_attention_heads * size_per_head,</span><br><span class="line">    ...</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这是对Q、K、V分别进行映射。虽然只是一次计算，但它代表着num_attention_heads次将Q、K、V分别映射到size_per_head维的空间中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># `query_layer` = [B, N, F, H]</span><br><span class="line">query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                   num_attention_heads, from_seq_length,</span><br><span class="line">                                   size_per_head)</span><br><span class="line"></span><br><span class="line"># `key_layer` = [B, N, T, H]</span><br><span class="line">key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                 to_seq_length, size_per_head)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这是为了方便下面的点乘计算相似度，而将Query和Key的值进行转置。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># `attention_scores` = [B, N, F, T]</span><br><span class="line">attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)</span><br><span class="line">attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                               1.0 / math.sqrt(float(size_per_head)))</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;attention_scores就是通过Q与K的乘法计算出的相似度。然后对其的每一维都除以size_per_head的平方根。论文中说第二步处理可以使模型获得更好的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># `attention_probs` = [B, N, F, T]</span><br><span class="line">attention_probs = tf.nn.softmax(attention_scores)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;通过softmax方法将Key与Query的相似度归一化为Value的权重。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># `value_layer` = [B, T, N, H]</span><br><span class="line">value_layer = tf.reshape(</span><br><span class="line">    value_layer,</span><br><span class="line">    [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line"># `value_layer` = [B, N, T, H]</span><br><span class="line">value_layer = tf.transpose(value_layer, [0, 2, 1, 3])</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;对Value进行转置来方便它和权重的点乘计算。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># `context_layer` = [B, N, F, H]</span><br><span class="line">context_layer = tf.matmul(attention_probs, value_layer)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;将Value和之前获得的权重进行点乘，计算出每一个head的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># `context_layer` = [B, F, N, H]</span><br><span class="line">context_layer = tf.transpose(context_layer, [0, 2, 1, 3])</span><br><span class="line"># `context_layer` = [B*F, N*V]</span><br><span class="line">context_layer = tf.reshape(</span><br><span class="line">  context_layer,</span><br><span class="line">  [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;将每一个head的结果首尾相接，连成num_attention_heads*size_per_head维的结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              ...)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;将attention层的结果由num_attention_heads*size_per_head维映射回hidden_size维。<br>&emsp;&emsp;至此，我们将Encoder中的Mulit-Head Attention层的工作流程介绍完毕。</p>
<h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h3><p>&emsp;&emsp;相较于attention层而言，FFNN层的结构则要简单的多，在本模型中，FFNN层的输入（即attention层的输出）被传入一个隐层节点数为3072的全连接网络中，最终以统一的hidden_size为维度输出。值得一提的是，隐层使用的激活函数是<a href="https://arxiv.org/pdf/1606.08415.pdf" target="_blank" rel="noopener">Gelu</a>，这是relu函数的一种更为平滑的版本，输出的结果是标准正态分布的累积分布函数值与输入值的乘积。<br>&emsp;&emsp;代码实现如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))</span><br><span class="line">return input_tensor * cdf</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这是gelu函数的代码，input_tensor是函数的输入，erf是误差函数，cdf是标准正态分布的累积分布函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">intermediate_output = tf.layers.dense(</span><br><span class="line">    attention_output,</span><br><span class="line">    intermediate_size,</span><br><span class="line">    activation=intermediate_act_fn,</span><br><span class="line">    kernel_initializer=create_initializer(initializer_range))</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这是节点数为intermediate_siza（3072），激活函数为gelu的隐层。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这是节点数为hidden_size的输出层。</p>
<h3 id="Add-amp-Norm-残差连接和层正则化"><a href="#Add-amp-Norm-残差连接和层正则化" class="headerlink" title="Add&amp;Norm 残差连接和层正则化"></a>Add&amp;Norm 残差连接和层正则化</h3><p>&emsp;&emsp;<a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">layer normalization</a>是<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch Normalization</a>的一种优化版本，具体原理可以查阅论文得知，在此不过多赘述。<br>&emsp;&emsp;代码实现如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def layer_norm(input_tensor, name=None):</span><br><span class="line">  &quot;&quot;&quot;Run layer normalization on the last dimension of the tensor.&quot;&quot;&quot;</span><br><span class="line">  return tf.contrib.layers.layer_norm(</span><br><span class="line">      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这是本模型中的layer_norm的函数实现，可以看见主要是调用了tensorflow的layer_norm()函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">attention_output = layer_norm(attention_output + layer_input)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这是attention层后的残差连接和层正则化。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layer_output = layer_norm(layer_output + attention_output)</span><br></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;这是FFNN层后的残差连接和层正则化。</p>
<h2 id="Input-Embedding"><a href="#Input-Embedding" class="headerlink" title="Input Embedding"></a>Input Embedding</h2><p>&emsp;&emsp;本模型中的Input Embedding由下图中的三部分组成：<br><img src="https://gbj0fw.dm.files.1drv.com/y4mq6s0Oo0YyDtapQiukmzVWLIMgdoxMhy9Xl_dpGdoNb7g4diA98caGRZCjLcSda4NOoBfS6rJ3NTK7dP1Ld02_2eje3muzOQKoJSWo5PdwAKZ_pGlrJ-t0SYQ7FBiEGYxGnU_druRX8j16V3r5VddiJGpDZQTZ4zxKsXJ2d3yu3itUSfBLvRl8UwOuwz6hxW2ApJAHaZPws5Mli6fyvGqJg?width=1009&amp;height=362&amp;cropmode=none" alt=""><br>&emsp;&emsp;第一部分为普通的word embedding，由训练得到；第二部分为segment emdedding，因为此模型的输入有可能是不同数量的句子，我们使用这一部分来区分不同的句子，同样由训练得出；第三部分为position embedding，由于此模型没有使用RNN网络，因此，为了保存词语之间的先后顺序关系，我们使用这一部分来记录每一个词语在输入中的相对位置。</p>
<h2 id="Pre-Training-Tasks"><a href="#Pre-Training-Tasks" class="headerlink" title="Pre-Training Tasks"></a>Pre-Training Tasks</h2><p>&emsp;&emsp;Bert预训练过程的loss函数主要由两个部分组成：</p>
<h3 id="Masked-LM"><a href="#Masked-LM" class="headerlink" title="Masked LM"></a>Masked LM</h3><p>&emsp;&emsp;在此语言模型中，为了不提前“泄露”需要预测的内容，在数据预处理部分，就Mask了15%的词汇，作为需要预测的目标。针对这15%的词汇，做出一下处理：</p>
<ol>
<li>80%的情况下，用[mask]来代替选中的词语，如 my dog is hairy →m y dog is [MASK]；</li>
<li>10%的情况下，用一个随机词语来代替选中的词语，如 my dog is hairy → mydog is apple；</li>
<li>10%的情况下，选中的词语维持不变，如 my dog is hairy → my dogi s hairy。</li>
</ol>
<p>&emsp;&emsp;第2，3种情况是为了防止模型对[mask]字符产生依赖，变得只有看见[mask]字符时才能准确预测结果，因为面对具体任务的输入中并不会由[mask]字符。<br>&emsp;&emsp;在loss函数中，只有这15%的被选中的单词的预测的准确率会被统计。</p>
<h3 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h3><p>&emsp;&emsp;在预训练的过程中，每个输入包含2个句子：句子A和句子B。在50%的情况下，句子B是句子A的下一个句子；在50%的情况下，句子B是一个随机的句子。<br>&emsp;&emsp;在这一任务中，我们通过预测句子间的连接关系，来学习句子内的逻辑联系。</p>
</div><div class="tags"><a href="/fanyangfanyang.github.io/tags/NLP/">NLP</a><a href="/fanyangfanyang.github.io/tags/Bert/">Bert</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/fanyangfanyang.github.io/2018/12/13/使用Hexo+Github搭建个人博客/" class="next">使用Hexo+Github搭建个人博客</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#摘要"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer"><span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bert"><span class="toc-text">Bert</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer-Encoder-Block"><span class="toc-text">Transformer Encoder Block</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-Head-Attention"><span class="toc-text">Multi-Head Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feed-Forward"><span class="toc-text">Feed Forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Add-amp-Norm-残差连接和层正则化"><span class="toc-text">Add&amp;Norm 残差连接和层正则化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Input-Embedding"><span class="toc-text">Input Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pre-Training-Tasks"><span class="toc-text">Pre-Training Tasks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Masked-LM"><span class="toc-text">Masked LM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Next-Sentence-Prediction"><span class="toc-text">Next Sentence Prediction</span></a></li></ol></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/fanyangfanyang.github.io/2018/12/14/结合源码理解Bert/">结合源码理解Bert</a></li><li class="post-list-item"><a class="post-list-link" href="/fanyangfanyang.github.io/2018/12/13/使用Hexo+Github搭建个人博客/">使用Hexo+Github搭建个人博客</a></li><li class="post-list-item"><a class="post-list-link" href="/fanyangfanyang.github.io/2018/12/13/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/fanyangfanyang.github.io/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/fanyangfanyang.github.io/tags/Bert/" style="font-size: 15px;">Bert</a> <a href="/fanyangfanyang.github.io/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/fanyangfanyang.github.io/tags/Github/" style="font-size: 15px;">Github</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/fanyangfanyang.github.io/archives/2018/12/">十二月 2018</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="http://sonack.github.io/" title="Sonack's Blog" target="_blank">Sonack's Blog</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><!--if theme.baidusitemap--><!--  a(href=config.root+"baidusitemap.xml")= __("baidusitemap")--><!--  |  |  --><!--if theme.feed--><!--  a(href=config.root+"atom.xml")= __("rss")--><!--  |  |  --><a href="/fanyangfanyang.github.io/about/">关于</a></p><p><span> Copyright &copy;<a href="/fanyangfanyang.github.io/." rel="nofollow">Fan Yang.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?00a42b33f9acbb41b9f74df092a629f8";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/fanyangfanyang.github.io/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/fanyangfanyang.github.io/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>